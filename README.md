# Dr.Doctor Backend

A web scraping system for collecting doctor and hospital data from Pakistani healthcare platforms (Marham, Oladoc) and storing it in MongoDB for downstream processing.

## Project Overview

This project is part of a larger system that:
- Scrapes doctor and hospital data from multiple Pakistani healthcare platforms
- Stores structured data in MongoDB
- Supports location-based data collection and enrichment
- Provides data export/import utilities for collaboration

## Project Structure

```
dr_doctor_backend/
â”œâ”€â”€ dr_doctor_scraper/          # Main scraper package
â”‚   â”œâ”€â”€ scrapers/               # Scraper implementations
â”‚   â”‚   â”œâ”€â”€ base_scraper.py    # Base scraper with Playwright wrapper
â”‚   â”‚   â”œâ”€â”€ marham_scraper.py  # Marham platform scraper
â”‚   â”‚   â”œâ”€â”€ oladoc_scraper.py  # Oladoc platform scraper
â”‚   â”‚   â”œâ”€â”€ models/            # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ database/          # MongoDB client wrapper
â”‚   â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â””â”€â”€ tools/             # Export/import utilities
â”‚   â”œâ”€â”€ run_scraper.py         # Main entry point
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ README.md              # Detailed scraper documentation
â”œâ”€â”€ mongo_test.py              # MongoDB connection test script
â””â”€â”€ README.md                  # This file
```

## Quick Start

1. **Navigate to the scraper directory:**
   ```powershell
   cd dr_doctor_scraper
   ```

2. **Set up virtual environment:**
   ```powershell
   python -m venv .venv
   .venv\Scripts\Activate.ps1
   ```

3. **Install dependencies:**
   ```powershell
   pip install -r requirements.txt
   playwright install
   ```

4. **Configure environment:**
   - Copy `.env.example` to `.env` (if it exists)
   - Set `MONGO_URI` in `.env` file

5. **Run the scraper:**
   ```powershell
   python run_scraper.py --site marham --limit 5
   ```

For detailed documentation, see [dr_doctor_scraper/README.md](dr_doctor_scraper/README.md).

## Technologies

- **Python 3.10+**
- **Playwright** - Browser automation
- **BeautifulSoup** - HTML parsing
- **Pydantic** - Data validation and models
- **MongoDB** - Data storage
- **Loguru** - Logging

## Current Phase

**Phase 1: Scraper Refinement & Testing** ðŸ”„

We are currently refining the Marham scraper to capture comprehensive doctor and hospital data, with testing planned at increasing scales (5 â†’ 100 â†’ all Karachi hospitals).

## Development Status

### Completed âœ…
- âœ… Marham scraper (hospital-first approach) - Created and tested
- âœ… Modular scraper architecture (parsers, enrichers, collectors, mergers)
- âœ… Oladoc scraper (basic implementation)
- âœ… MongoDB integration with upsert support
- âœ… Data export/import utilities
- âœ… Project structure and file organization
- âœ… Bidirectional doctor-hospital relationships
- âœ… Comprehensive doctor data capture (interests, services, diseases, symptoms)
- âœ… Queue-based dynamic work distribution for all steps
- âœ… Page tracking and retry system

### In Progress ðŸ”„
- ðŸ”„ Testing and validation of bidirectional relationships
- ðŸ”„ Performance optimization

### Planned ðŸ“‹
- ðŸ“‹ Testing with 100 hospitals
- ðŸ“‹ Scaling to all hospitals in Karachi
- ðŸ“‹ Additional platform scrapers
- ðŸ“‹ User queries and doctor replies collection
- ðŸ“‹ Patient reviews and ratings collection
- ðŸ“‹ Autonomous scraping/crawling system

## Project Roadmap

This project follows a phased development approach to build a comprehensive healthcare data platform with AI-powered chatbot capabilities.

### Phase 1: Scraper Refinement & Testing (Current) ðŸ”„

**Goal**: Build and validate a robust scraper that captures comprehensive doctor and hospital data.

- âœ… Create and test Marham base scraper
- ðŸ”„ Refactor to include comprehensive doctor and hospital data fields
- ðŸ“‹ Test with 5 hospitals
- ðŸ“‹ Test with 100 hospitals
- ðŸ“‹ Scale to all hospitals in Karachi
- ðŸ“‹ Validate data quality and completeness

**Deliverables**: Production-ready Marham scraper with complete data capture

---

### Phase 2: Data Collection Expansion ðŸ“‹

**Goal**: Expand data collection to include user interactions and reviews, and make the system autonomous.

- ðŸ“‹ Refactor codebase to gather complete hospital data across all available fields
- ðŸ“‹ Collect user queries and doctor replies from platform Q&A sections
- ðŸ“‹ Collect patient reviews and ratings
- ðŸ“‹ Implement autonomous scraping/crawling with scheduling and error recovery
- ðŸ“‹ Add monitoring and alerting for scraper health

**Deliverables**: Autonomous scraping system with comprehensive data collection

---

### Phase 3: Multi-Platform Integration ðŸ“‹

**Goal**: Integrate data from multiple healthcare platforms and create a unified data model.

- ðŸ“‹ Complete Oladoc scraper integration
- ðŸ“‹ Add scrapers for additional platforms (AKU, etc.)
- ðŸ“‹ Build data merging and deduplication pipeline
- ðŸ“‹ Create unified data model across all platforms
- ðŸ“‹ Implement data quality validation and normalization

**Deliverables**: Multi-platform data aggregation system with unified schema

---

### Phase 4: ML & NLP Foundation ðŸ“‹

**Goal**: Build the AI foundation for intelligent patient assistance.

- ðŸ“‹ Build Rasa/Reg model for chatbot foundation
- ðŸ“‹ Train models on collected doctor/hospital data
- ðŸ“‹ Implement natural language understanding for medical queries
- ðŸ“‹ Create intent classification and entity extraction
- ðŸ“‹ Build conversation flow management

**Deliverables**: Functional chatbot with medical domain knowledge

---

### Phase 5: Advanced Features ðŸ“‹

**Goal**: Add intelligent features for enhanced user experience and data collection.

- ðŸ“‹ Text-to-speech integration for accessibility
- ðŸ“‹ Local language detection via user location (Urdu, regional languages)
- ðŸ“‹ Location-based hospital/clinic finder (20km radius)
- ðŸ“‹ ML-based intelligent scraper/crawler for data lake
- ðŸ“‹ Automated data cleaning pipeline
- ðŸ“‹ Model training and inference pipeline
- ðŸ“‹ Real-time data enrichment and updates

**Deliverables**: Intelligent features with ML-powered data collection

---

### Phase 6: Frontend Development ðŸ“‹

**Goal**: Build user and doctor-facing interfaces with advanced medical features.

#### User-Facing Frontend (Patient Portal)
- ðŸ“‹ Patient registration and profile management
- ðŸ“‹ Hospital/doctor search and filtering
- ðŸ“‹ Appointment booking interface
- ðŸ“‹ Test result scanning and interpretation
- ðŸ“‹ Chat interface with AI assistant
- ðŸ“‹ Medical history tracking

#### Doctor-Facing Frontend (Doctor Dashboard)
- ðŸ“‹ Doctor profile management
- ðŸ“‹ Patient history and records
- ðŸ“‹ Evidence-Based Medicine (EBM) references and integration
- ðŸ“‹ Test results analysis and interpretation tools
- ðŸ“‹ AI-assisted diagnosis suggestions
- ðŸ“‹ Prognosis and treatment planning tools
- ðŸ“‹ Patient communication interface

**Deliverables**: Complete frontend applications for patients and doctors

---

### Phase 7: Integration & Deployment ðŸ“‹

**Goal**: Integrate all components and deploy to production.

- ðŸ“‹ End-to-end integration of all components
- ðŸ“‹ Comprehensive testing (unit, integration, E2E)
- ðŸ“‹ Performance optimization and scaling
- ðŸ“‹ Security audit and compliance (HIPAA considerations)
- ðŸ“‹ Production deployment and monitoring
- ðŸ“‹ Documentation and user guides
- ðŸ“‹ Maintenance and support plan

**Deliverables**: Production-ready, fully integrated healthcare platform

---

## Notes

- The scraper uses a hospital-first approach for Marham, collecting hospitals first then doctors within each hospital
- Data is stored in MongoDB with unique indexes on `profile_url` (doctors) and `name+address` (hospitals)
- Export/import tools are available in `scrapers/tools/` for sharing database snapshots
- The roadmap is iterative - phases may overlap and priorities may shift based on learnings and requirements
