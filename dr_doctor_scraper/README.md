# Dr.Doctor Scraper

Production-ready web scraping system for collecting structured doctor and hospital data from Pakistani healthcare platforms (Marham, Oladoc) and storing it in MongoDB.

## ğŸš€ Quick Start

```powershell
# 1. Setup
cd dr_doctor_scraper
python -m venv .venv
.venv\Scripts\Activate.ps1
pip install -r requirements.txt
playwright install

# 2. Configure
copy .env.example .env
# Edit .env and set MONGO_URI

# 3. Run
python run_scraper.py --site marham --threads 4 --limit 10 --test-db
```

## ğŸ“š Documentation

**ğŸ“– [Documentation Index](DOCUMENTATION.md)** - Complete guide to all documentation

### Essential Docs
- **[API Reference](API_REFERENCE.md)** - Complete function and class documentation
- **[Commands Guide](COMMANDS.md)** - All commands, workflows, and usage examples
- **[Changelog](CHANGELOG.md)** - Complete history of all changes and improvements

### Feature Guides
- **[Testing Guide](TESTING.md)** - Testing environment and best practices
- **[Multi-threading Guide](MULTITHREADING.md)** - Parallel processing guide
- **[Step Guide](STEP_GUIDE.md)** - Step-by-step workflow execution

## ğŸ—ï¸ Architecture

### Modular Structure

```
scrapers/
â”œâ”€â”€ base_scraper.py              # Browser management (Playwright)
â”œâ”€â”€ marham_scraper.py            # Single-threaded Marham scraper
â”œâ”€â”€ marham/
â”‚   â”œâ”€â”€ multi_threaded_scraper.py    # Multi-threaded wrapper
â”‚   â”œâ”€â”€ parsers/                     # HTML parsing
â”‚   â”‚   â”œâ”€â”€ hospital_parser.py
â”‚   â”‚   â””â”€â”€ doctor_parser.py
â”‚   â”œâ”€â”€ enrichers/                   # Data enrichment
â”‚   â”‚   â””â”€â”€ profile_enricher.py
â”‚   â”œâ”€â”€ collectors/                  # Data collection
â”‚   â”‚   â””â”€â”€ doctor_collector.py
â”‚   â”œâ”€â”€ handlers/                    # Business logic
â”‚   â”‚   â””â”€â”€ hospital_practice_handler.py
â”‚   â””â”€â”€ mergers/                     # Data merging
â”‚       â””â”€â”€ data_merger.py
â”œâ”€â”€ crawler/                      # Web crawler module
â”‚   â”œâ”€â”€ web_crawler.py            # Main crawler
â”‚   â”œâ”€â”€ multi_threaded_crawler.py    # Multi-threaded crawler
â”‚   â”œâ”€â”€ distributed_crawler.py    # Distributed crawler
â”‚   â”œâ”€â”€ content_analyzer.py       # Content analysis
â”‚   â”œâ”€â”€ sitemap_parser.py         # Sitemap.xml parser
â”‚   â”œâ”€â”€ js_detector.py            # JavaScript detection
â”‚   â”œâ”€â”€ asset_discovery.py        # Asset discovery
â”‚   â”œâ”€â”€ site_map_generator.py     # Site map generation
â”‚   â”œâ”€â”€ crawler_config.py         # Configuration
â”‚   â”œâ”€â”€ utils.py                  # Utilities
â”‚   â””â”€â”€ run_crawler.py            # CLI entry point
â”œâ”€â”€ database/
â”‚   â””â”€â”€ mongo_client.py          # MongoDB operations
â”œâ”€â”€ models/                       # Pydantic data models
â”‚   â”œâ”€â”€ doctor_model.py
â”‚   â”œâ”€â”€ hospital_model.py
â”‚   â””â”€â”€ crawl_model.py           # Crawler models
â””â”€â”€ utils/                        # Utility functions
    â”œâ”€â”€ url_parser.py
    â””â”€â”€ parser_helpers.py
```

### Four-Step Workflow

1. **Step 0**: Collect all cities from hospitals page (simple HTTP requests)
2. **Step 1**: Collect hospitals from listing pages (per city)
3. **Step 2**: Enrich hospitals and collect doctor URLs
4. **Step 3**: Process and enrich doctor profiles

Each step is resumable and can be run independently.

## ğŸ¯ Features

- âœ… **Multi-threading**: 4-8x faster with parallel processing
- âœ… **Resumable**: Continue from where you left off
- âœ… **Modular**: Reusable, testable components
- âœ… **Comprehensive**: Captures 50+ data fields per doctor/hospital
- âœ… **Bidirectional Relationships**: Doctors â†” Hospitals with full details
- âœ… **Robust**: Error handling, retries, validation
- âœ… **Testable**: Separate test database support
- âœ… **Documented**: Complete API and command reference
- âœ… **Web Crawler**: General-purpose crawler for site mapping and content analysis

## ğŸ“Š Data Captured

### Doctors
- Basic info (name, URL, specialty, platform)
- Qualifications (institute, degree)
- Experience (years, work history)
- Services, diseases, symptoms, **interests**
- Professional statement, patient stats
- **Hospital affiliations with fees/timings** (bidirectional relationship)
- Private practice information
- Contact details

### Hospitals
- Basic info (name, URL, address, location)
- Founded year, achievements
- Clinical departments, procedures
- Facilities, support services
- Fee ranges, contact numbers
- **Doctor lists with details** (bidirectional relationship)

## ğŸ› ï¸ Technologies

- **Python 3.10+**
- **Playwright** - Browser automation
- **BeautifulSoup** - HTML parsing
- **Pydantic** - Data validation
- **MongoDB** - Data storage
- **Loguru** - Logging

## ğŸ“– Usage Examples

### Basic Scraping

```powershell
# Single-threaded
python run_scraper.py --site marham

# Multi-threaded (4 threads)
python run_scraper.py --site marham --threads 4

# With limit (testing)
python run_scraper.py --site marham --limit 100 --threads 4
```

### Step-by-Step

```powershell
# Step 0: Collect cities (no threads needed, uses HTTP requests)
python run_scraper.py --site marham --step 0

# Step 1: Collect hospitals (per city)
python run_scraper.py --site marham --threads 4 --step 1

# Step 2: Enrich hospitals
python run_scraper.py --site marham --threads 4 --step 2

# Step 3: Process doctors
python run_scraper.py --site marham --threads 4 --step 3
```

### Testing

```powershell
# Use test database
python run_scraper.py --site marham --limit 10 --test-db --threads 2

# Validate results
python scripts/validate_data.py --test-db

# Verify relationships
python scripts/verify_db_relationships.py

# Test relationships with sample data
python scripts/test_relationships.py

# Analyze performance
python scripts/analyze_logs.py --limit 10
```

### Web Crawler

The web crawler is a general-purpose tool for discovering and analyzing website content. It can:
- Discover all URLs on a website
- Create hierarchical site maps
- Analyze content types and data patterns
- Search for keywords to identify relevant pages
- Discover images, CSS, JS, and other assets
- Parse sitemap.xml files
- Detect JavaScript-rendered content

```powershell
# Single-threaded crawl
python scrapers/crawler/run_crawler.py --url https://www.marham.pk --keywords doctor,hospital

# Multi-threaded crawl (8 threads)
python scrapers/crawler/run_crawler.py --url https://www.marham.pk --threads 8 --max-depth 5

# Crawl with specific keywords and limits
python scrapers/crawler/run_crawler.py --url https://www.aku.edu --keywords doctor,physician,department --max-pages 100

# Distributed crawling (multiple instances)
python scrapers/crawler/run_crawler.py --url https://www.oladoc.com --distributed --instance-id crawler-1

# Crawl with all features disabled (faster)
python scrapers/crawler/run_crawler.py --url https://www.marham.pk --no-sitemap --no-js-detection --no-assets

# Use test database
python scrapers/crawler/run_crawler.py --url https://www.marham.pk --test-db --threads 4
```

**Crawler Options:**
- `--url`: Starting URL(s), comma-separated for multiple URLs
- `--keywords`: Keywords to search for, comma-separated
- `--max-depth`: Maximum crawl depth (default: unlimited)
- `--max-pages`: Maximum number of pages to crawl (default: unlimited)
- `--threads`: Number of threads for parallel crawling (default: 1)
- `--distributed`: Enable distributed crawling mode
- `--instance-id`: Instance ID for distributed crawling
- `--no-sitemap`: Disable sitemap.xml parsing
- `--no-js-detection`: Disable JavaScript rendering detection
- `--no-assets`: Disable asset discovery
- `--no-robots`: Don't respect robots.txt
- `--delay`: Delay between requests in seconds (default: 0.5)
- `--headless`: Run browser in headless mode (default: True)
- `--no-headless`: Run browser with visible UI
- `--test-db`: Use test database

**Crawler Output:**
- All discovered pages stored in `crawled_pages` collection
- Site maps stored in `site_maps` collection
- Assets stored in `crawled_assets` collection
- Each page includes: URL, title, depth, content type, keywords found, links, assets

## ğŸ”§ Scripts

- `run_scraper.py` - Main scraper entry point
- `scrapers/crawler/run_crawler.py` - Web crawler entry point
- `scripts/analyze_logs.py` - Log analysis and statistics
- `scripts/validate_data.py` - Data validation
- `scripts/export_and_clear_db.py` - Database export
- `scripts/log_diagnostics.py` - Detailed log diagnostics
- `scripts/verify_db_relationships.py` - Verify doctor-hospital bidirectional relationships
- `scripts/test_relationships.py` - Test relationships with sample data

## ğŸ“ Project Status

**Current Phase**: Phase 1 - Scraper Refinement & Testing âœ…

See [Project Roadmap](../README.md#project-roadmap) for full development plan.

## ğŸ¤ Contributing

1. Use test database for development: `--test-db`
2. Run validation after changes: `python scripts/validate_data.py --test-db`
3. Check logs for issues: `python scripts/log_diagnostics.py`
4. Follow modular architecture patterns

## ğŸ“„ License

[Your License Here]

## ğŸ”— Links

- [API Reference](API_REFERENCE.md)
- [Commands Guide](COMMANDS.md)
- [Changelog](CHANGELOG.md)
